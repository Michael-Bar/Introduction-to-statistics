---
title: "AMP - Designing an RCT "
author: '[Michael Barber](mailto:mike.barber@oneacrefund.org)'
date: "September 4th, 2017"
output:
  html_document:
    code_folding: hide
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 6
    toc_float: yes
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 6
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2) # this is a nice plotting package we will use
library(cowplot) # this gives us extra options when plotting 



```


# Overview

<!---
Set code folding to show because you want them to see the code and interact with it
--->

The randomized controlled trial (RCT) is a key tool for any analyst, what separates the RCT from other statistical tools is its ability to define causality. Whereas most statistical techniques can only tell us about *correlations*, the RCT can tell us about *causations*, i.e. whether A causes B, or B causes A. As such, the RCT is central to proving what does and does not work for our farmers.

This lesson will outline the design principles of RCTs, you should consult this lesson before every RCT to ensure you have considered each and every aspect of an effective and cost-efficient RCT. Later lessons will deal with how to analyse RCT results. Note that not all trials will be a full-fledged RCT, there will also be simpler trials run in earlier phases to obtain estimates of logistical feasibility and impact (etc). These are described  [here ](https://docs.google.com/presentation/d/14CpKbrEtFOzPfYTmfwKZgnpmANYbKC8YmX1ZxTRZsQs/edit#slide=id.g204cb16ee8_0_0). 

## Objectives:

After this lesson you should: 

* Know steps to take when designing an RCT and the order to follow them

* Be able to judge whether an RCT is feasible given constraints on sample sizes and measurement strategies. 

We will break an RCT into it's parts (hypothesis, randomization, and measurement) and examine what is required from each part to make an effective RCT.

## Glossary and key terms:

* **Intervention**: An intervention is the product or program innovation you are testing in a trial, e.g. repayment incentives, or new seed varieties.

* **Co-variate** : Other variables in a study that may correlate with your outcome. E.g. if you are measuring the effect of solar lighting on farmer repayment then maize yield might be an important co-variate that influences our outcome. 

* **Cluster**: A group of individuals sharing some co-variates. An example at One Acre Fund would be a group or site. 

* **Randomisation unit**: The level at which randomization (and analysis) occurs, this could be the individual, the group, or the site (or other levels)

* **Outliers**: Data points which do not seem to follow the main distribution, note that this is often very subjective.

* **Scientific writing**: A style of writing that makes work transparent and communicable to other analysts

* **Treatment group**: Those individuals or clusters that have been assigned to an intervention

* **Control group**: Those individuals or clusters that have been assigned to not receive an intervention


# Principles of RCTs

<!---
What are you trying to accomplish with this section? I don't think these terms will make sense the first time through for readers and you don't then bring them back in the randdomization section. I suggest combining these sections and motivating it from the perspective of an OAF analyst who has an impact question they're trying to answer. That question leads us to "how do we measure that impact" and why we can't just let farmers self-select into our study groups, etc. That should give you a path to introduce all of those topics.
-->

Each RCT will have some shared elements:

* [**Hypothesis**](#hypotheses): A proposition (or statement) to evaluate.

* [**Randomization**](#randomisation-in-rcts): The random assignment of individuals or groups to a treatment or control group

* **A control group**: a randomly selected group assigned to *not* receive the intervention

* **A treatment group** a randomly selected group assigned to receive the intervention. It is possible to have multiple treatment groups in an RCT, one for each intervention. 

* [**Measurement**](#measurement): The act of recording observations from the outcome variable

* [**Data checking**](#data-checking): Checking data and distributions during a trial for consistency and quality

These are combined into an RCT. An RCT will allow us to assign causality and determine the impact of an intervention if we hold every other variate constant. Let us examine each of these in turn. 

# Randomisation in RCTs

Why do we randomize in an RCT? And why would we not want farmers to self-select whether they are treatment or control? It essentially comes down to eliminating **bias** and distributing **co-variates** equally. 

Co-variates are factors that might influence your outcome variable, for example, farmer location, farmer income, soil type (etc). Bias, in proper statistics jargon, *means when your sample is substantially different from your popultion*. In an RCT we are assuming our sample is representative of our population, deviations from this assumption can lead us to an incorrect understanding of our population and generating conclusions that look robust but are actually invalid! The two types of bias we encounter at this stage of an RCT are:

* **Randomization bias** - bias can be due to poor randomization resulting in *unbalanced T/C groups* (e.g. we have more control than treatment farmers in District X).

<!---
Randomization bias is more concerned with statistical balance than numerical balance, yes? Please clarify.
--->

* **Selection bias** - bias would also result if we were to allow farmers to assign themselves to T/C groups. This is because there might be an unobservable reason why farmers are choosing T/C groups. For example, wealthier farmers might have more risk appetite and therefore select themselves into treatment groups more, 

    * Both of these will lead to what is called **confounding bias**, this means it will be difficult to untangle effects that are due to poor randomization and effects that are due to the actual intervention.
    
    
* However, if each participant has an equal chance of being randomly assigned to a T/C group then randomization will be free of bias. This will result in both observable (e.g. farmer location) and un-observable co-variates (such as risk appetite) being spread equally to T and C groups. It is this spreading of co-variates that allows us to understand causality. 

It is therefore important to select treatment and control groups *totally randomly*, the best way to achieve this is by letting R do the work for you, as in the below code:

<!---
I'd add a sentence that says something like, "continue to follow along with the code and comments below" so they know to read the comments as well. The comments are not intuitively where people look for the lesson to continue.

I'd also elaborate on what set.seed is doing and how it makes the results replicable. This could be a good space for a learnr example to have them generate random numbers with and without seeds to see that they get consistent results. I'd really stress this as they could be in a bad place if they don't freeze their randomization with set.seed.
--->

```{r sampling, comment=NA, warning=FALSE}

#lets first make up a fake list of farmer IDS from 1 to 1000 and 1000 random variables drawn from a normal dist. (called yield)
df = data.frame("OAFID"=seq(1,1000), "yield"=rnorm(1000))

#lets now make a function to do the work - you can copy paste this funciton into your own scripts
# it needs to be given a dataframe and a list of naming options
# Options might be "treatment" and "control", or if there are more than 2 options then it might be "Control", "treatment1", "treatment2", or "control", "sunking home", "sunking pro"
#note you will need to copy this bit directly below for it to work in your code
RCT_random = function(dataframey, values_to_add){
  dataframey$values_to_add[sample(1:nrow(dataframey), nrow(dataframey), FALSE)] <- rep(values_to_add)
  colnames(dataframey)[which(colnames(dataframey)=="values_to_add")] = "Status"
  return(dataframey) }


# We set a seed to make our results replicable. A seed means it will always draw numbers the same way. You can make up any number for the seed (9 is my example)
set.seed(9)
# so this will take the dataframe called "df" and randomly assign each ROW to "Treatment" or "control"
df_new = RCT_random(df, c("Treatment","Control"))

# so this will take the dataframe called "df" and randomly assign each ROW to either "treatment1", "treatment2" or "control"
df_new2 = RCT_random(df, c("Treatment1","Treatment2", "Control"))

```

We have now taken our original dataframe:

```{r showdf, comment=NA}
head(df)
```

And randomly assigned T and C status to them with my function "RCT_random":

```{r showdf2, comment=NA}
head(df_new)
```


We should now double-check our randomization to ensure it has proceeded as expected, to do this let us look at the summary statistics for Treatment and Control groups and make sure the "yield" variable is similar:

```{r hypolooksie}


ggplot(df_new, aes(x=yield, fill=Status)) + geom_density(alpha=.3) + xlab("Yield")



```


They look pretty similar! So far so good, randomization has been successful.

<!---
Add in a statistical test to evaluate balance. Visualizing them is a good start but not sufficient. What test should they use? How do they evaluate the test?

What should they do if the groups are not balanced? What degree of balance is good or not good? 

This is a good chance to illustrate the principle of p < 0.05 and type 1 error. If we're checking for balance on a large set of covariates, one of them is likely to be significant. That concept can segue into multiple hypothesis testing if you wanted.
--->

## Quick quiz

Let's look at some examples of randomisation strategies below and try to decide whether they are proper or improper randomisation:

1. Farmers are allowed to decide whether to be in a treatment or control group

2. Any farmer with a national ID number ending in an odd number is assigned to treatment, any farmer with an ID ending in an even number is assigned to control.

3. Farmers east of the OAF office are assigned to control, farmers west of the OAF office are assigned to treatment

4. We flip a coin to decide whether a farmer is control or treatment

Which of the above are truly randomized? Take a moment to think about this before expanding the answer below.

<div id="spoiler" style="display:none">


1. Farmers are allowed to decide whether to be in a treatment or control group. <span style="color:red"> This is not random, as farmers may have a reason for choosing a group which would lead to self-selection bias</span>

2.  Any farmer with a national ID number ending in an odd number is assigned to treatment, any farmer with an ID ending in an even number is assigned to control. <span style="color:red"> Almost random, but not quite! The issue here is that there might be some unknown factor associated with national ID that influences our results. If we 100% knew that national IDs were made randomly then this would be fine, however as we cannot know that we should use an alternative method </span>

3. Farmers east of the OAF office are assigned to control, farmers west of the OAF office are assigned to treatment. <span style="color:red">This is a terrible idea, this will likely lead to confounding bias as there are likely to be geographical differences between farmers east and west</span>

4. We flip a coin to decide whether a farmer is control or treatment, Heads means treatment, Tails means Control.<span style="color:blue">This is the only truly random system, as it is based *only* on chance</span>



</div>

<button title="Click to show answer" type="button" onclick="if(document.getElementById('spoiler') .style.display=='none') {document.getElementById('spoiler') .style.display=''}else{document.getElementById('spoiler') .style.display='none'}">Show/hide</button>

## Additional materials

* [Trial design training](https://docs.google.com/presentation/d/14CpKbrEtFOzPfYTmfwKZgnpmANYbKC8YmX1ZxTRZsQs/edit)
* [Randomization summary](https://drive.google.com/open?id=0B8h28h6EIayDWGpVa1hPcks2N3M)

## Summary

* We must properly randomize farmers into treatment and control groups to eliminate confounding bias and ensure that the sample we have (which we actually observe) is representative of the population we care about. Furthermore, we must distribute co-variates equally to be able to draw conclusions about causality.

* We achieve proper randomization by each and every farmer (or group or site) having an equal probability of being assigned to a treatment group. The best way to do this is by letting R do the work for you and using my RCT_random function above. 


# Cluster randomised trials

<!---
I'd approach this instead from the place of, "but what if you are unable to randomize at the individual level?" Elaborate on examples of why that might not be possible due to logistical constraints (often if the FO is involved then it's a constraint). Your training example is a good start.

Consider elaborating on spillover / contamination and why it matters for study design and inference.

I still don't understand your point about not randomizing at the individual level. Are you referring to a specific set of treatments that we wouldn't randomize at the individual level? Farmers being in groups doesn't mean that we can't randomize individuals to receive SMS, for example. Individual randomization should mean being a T farmer is uncorrelated with any feature that lead farmers to be one group over another. It's obviously not relevant to non-OAF farmers that aren't in any group but will be internally valid. Can you explain more what you mean?
--->

Often at One Acre Fund we will be running *cluster randomised trials* rather than *randomised controlled trials*. The difference between these two trials lies in the  **unit of inference** and the **unit of randomisation**. A traditional RCT seeks to assign treatment and control status at the individual level, whilst a cluster RCT seeks to assign treatment and control at a cluster level. A **cluster** is any group of individuals with shared characteristics that cannot be randomized away, or where there is significant risk of *contamination* of control/treatment status.

For example, if we were to assign some members of a lending group to treatment and some to control and test the impact of a new training on maize yields then there is a fair chance that communication between members of the same group would mean our control farmers could pick up the new training. This would weaken our ability to understand the impact of the training on yields. We would therefore want to randomize at the group or site level to minimize contamination (so all members of certain groups or sites are assigned T/C status).

A general issue with lending groups at OAF is that farmers self-select into groups, thus generating selection bias. It is therefore rare that we would ever randomize at the individual level. 

When analyzing data from a cluster RCT it is important to summarize the data by cluster. This means we match the unit of randomization to the unit of analysis. For example, if we randomize groups into treatment and control and want to understand the effect of an intervention on maize yields then we would want to summarize the farmers by group, so that we have the group average maize yield. I have included some R code below to show how to do this:

This is our raw data, broken down at the farmer level:

```{r summy, comment=NA}

#make some data
df = data.frame(FarmerID=seq(1,100), Maize_yield=rnorm(100,500,50),  group_name=c("farmers_first","farmers_second","farmers_third","farmers_last"), District=c("A","B"))

library(knitr)
kable(df[1:9,], format="markdown", align="c")



```

In total we have `r dim(df)[1]` rows of data. Now let us summarize it by group_name. I use the `dplyr` library to summarize the data. A couple notes on the code:

* We need load `dplyr` to access the %>% function.
* We call %>% "piping". We can pipe an object to a new function to simplify work and make the code easier to read.
* We indent after each %>% to create a stack of actions that we are applying to the data. To learn more, read [the dplyr vingette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html)
* So below we are taking df and piping it to a function that will group our data by `group name`. Once we have grouped our data we will pipe it again to smmarize which will try to calculate the mean of the `Maize_yield` column in our dataframe. This final object is then saved as sum_df

```{r summy2, comment=NA, warning=FALSE, message=FALSE}


library(dplyr) 
sum_df <- df %>% 
  group_by(group_name) %>% 
  summarize(
    yield.ave = mean(Maize_yield)
  )
  

#kable makes a pretty table in r markdown HTMLs
kable(sum_df, format="markdown", align="c")
```

We now have the average maize yield for each of our four groups ("farmers_first", "farmers_second", "farmers_third", "farmers_last"). We have only directed R to summarize `Maize_yield` so we no longer have the other columns in our dataset. However, the other columns were `ID` and `District`. Taking the average of these variables would have been meaningless. Finally, be aware that if one of our numeric columns were set to class "character" or class "factor" rather than class "numeric" then it would also fail to average them. 

<!---
I'd prefer that you show them the full code for using summarize_each when there are character variables. You can show them the right code and say, "if we didn't include -c(District), district would show up as NA because it's a character."
--->

Note that we can also manually manipulate our data if we want to specifically create new metrics:

```{r manip2, warning=FALSE, message=FALSE, comment=NA}

#library(plyr) 

#this will take df and group it by "group name". It will then create a new metric called "average_yield" which is the mean of maize_yield. Likewise it will create a column called "stdev_yield" that will have our standard deviations

sum_df2 <- df %>%
  group_by(group_name) %>%
  summarize(
    average_yield= mean(Maize_yield), 
    stdev_yield= sd(Maize_yield)
  ) %>% 
  as.data.frame()

kable(head(sum_df2))
                  
                  


```

This means that the sample size of 100 farmers in 4 groups is not 100! The reason is that under an RCT we assume that each individual has equal information gain, however, if farmer variables are correlated within groups then we already know something about the second farmer in the cluster just by examining the first farmer. This means the second farmer in our group has less information than the first, and so the third, fourth, fifth (etc) will have less information than each farmer preceding it. This drastically reduces our sample size because again, we violate the assumption of equal information for every individual. 

## Quick quiz

Let's look at some trial hypotheses and decide what level (farmer, group, FO, district) we should randomise at:

1. Trialing incentives for Field officers to improve farmer repayment

2. Trialing health insurance interventions to see if it improves farmer welfare

3. Trialing new planting practices taught by FOs


Take a moment to think about this before expanding the answer below.

<div id="spoiler2" style="display:none">


1. Trialing incentives for Field officers to improve farmer repayment <span style="color:red"> We would want to trial this at the FO/site level as we are testing an intervention for FOs </span>

2.   Trialing health insurance interventions to see if it improves farmer welfare <span style="color:red"> In theory, this could be trialed at the farmer level as there is no risk of spillover. However, due to morale and logistics this would probably want to be trialed at the site level so that farmers do not feel the trial is unfair </span>

3. Trialing new planting practices taught by FOs <span style="color:red"> Again, this would want to be randomized at the FO level as it would be the FOs doing the instruction  </span>

</div>

<button title="Click to show answer" type="button" onclick="if(document.getElementById('spoiler2') .style.display=='none') {document.getElementById('spoiler2') .style.display=''}else{document.getElementById('spoiler2') .style.display='none'}">Show/hide</button>

<!---
You need to introduce ICC concept more. I'd also change the inputs to the ICC_CI function so they're more human friendly. Make X groupVar and y outcomeVar.
--->

If necessary, we can directly calculate the ICC value for a study to determine where to randomize, remember our un-summarized dataframe from earlier:

```{r icyy0}

kable(head(df))
```

We can calculate the ICC using the snippet of code below: 
```{r iccy, comment=NA}

library(ICC)


# this is my function. It will calculate the confidence intervals for ICC
#it needs two character strings, x and y, which are the group level and the outcome variable column names in ""
# it also needs the dataframe which has column names x y
ICC_CI <- function(x,y, dataf){
  si = round(dim(dataf)[1]*0.66)
  values_f <- c()
  for(i in seq(1:300)){
  
  samp_f = dataf[sample(nrow(dataf), si), ]
  
  x_f = ICCbare(x,y,samp_f)
  
  values_f <- c(values_f, x_f)
  }
  # note that 1.96StDevs = 95% confidence interval bounds in a normal dist.
   ret = data.frame("Mean ICC" = round(mean(values_f, na.rm=TRUE),3), "CI" = round(1.96*sd(values_f, na.rm=TRUE),3))
   
   ret$Significant = ifelse(ret$Mean.ICC > ret$CI, "Y", "N")
  return( ret)
  
  }
#using kable to make the dataframe that is returned look pretty
ICC_CI("group_name", "Maize_yield", df)


```

We can see from this calculation that our ICC between farmers in the same lending group is `r round(ICC_CI("group_name", "Maize_yield", df)$Mean.ICC,3)` +- `r round(ICC_CI("group_name", "Maize_yield", df)$CI,3)`. As the confidence intervals (CI) cross zero we can see that this is not significant (hence "Significant" is "N"). This means that randomizing at the group level is fine. Had our ICC been significant ("Significant" = "Y") then we would want to repeat the calculation at the site level. 

In practice, at One Acre Fund we tend to assume an ICC of 1.0 and use the number of clusters (groups or sites) as the sample size (i.e. we calculate the sample size for the number of groups/sites needed, not the number of farmers). This is often the safest and easiest way to proceed. Using this method, our sample size for the above example would be only 4 (instead of 100). Sample size calculation methods with a determined ICC can be found in the upcoming FAQ. 

<!---
This is not true. There's more nuance than you're presenting. ICC can guide to a more precise understanding of the balance between groups and observations within groups. 

Summarizing the data to the cluster level simplifies the analysis but that's not the only option. As I mentioned in the gDoc comments, I think you should explain how the intuition of the ICC is reflected in the analysis (clustering SD because otherwise SD can be artificially low). We're not required to summarize to the cluster level.
--->

## Summary

<!----
I've made summary a ## instead of > so that someone can jump straight to it
--->

* Cluster-level effects can compromise an RCT. It is important to randomize and analyze at the correct level!

* You can estimate the ICC using historical data and my function to find the level at which ICC is lowest.

* It is likely that almost all studies will be done at the group or FO level. 


# Hypotheses

<!---
Move hypothesis before cluster RCT because it's relevant to both clustered and non-clustered RCTs? Your segue makes it seem like clusters is all we'll ever work with. Clustered RCTs is a subset of the RCT practice.
--->

Once we know how to randomize our clusters, we need a strong hypothesis to test. We will be testing our hypothesis by comparing our treatment and control groups. Each hypothesis has two parts:

* Null hypothesis (H0) : The null hypothesis usually states that there is *no difference* between treatment and control groups. (To put this another way, we're saying our treatment outcome average will be statistically *the same* as our control outcome average.)

* Alternative hypothesis (H1): The alternative hypothesis states that *there is a difference* between treatment and control groups. (In other words, the treament outcome average will be statistically greater than or less than the control outcome average.)

To give an example:

* Null hypothesis: Farmer groups who adopt solar will not have a higher final repayment percentage. 

<!---
This is a tricky example because adopting solar is self selection. A trial hypothesis would be more along the lines of farmers that are given the chance to adopt a solar light....

Maybe find another example that doesn't involve adoption? 

I'd also tie the hypothesis to the randomization method because we start with our question and then randomize. Move hypothesis formation before randomization and show how if we have question A then we randomize like so, if we have question B, we might randomize like so. Feel free to reference the other materials I've linked you to.

Hold over note from before: the hypotheses and randomization could all be at the individual level.
--->

* Alternative hypothesis: Farmer groups who adopt solar will have a higher final repayment percentage. 


To test this, we will randomly assign some farmers to get solar, and some farmers to not get solar, and then measure the final percentage of loans repaid in each group. However, there are a number of ways we can test for differences between treatment and control groups, we could look for differences in group average repayment, median repayment, or maximum repayment. A good hypothesis, therefore, will state what difference we are looking for (note that this hypothesis will need to be paired to an appropriate hypothesis test - lesson 3). 

<!---
Are you certain you can compare median outcomes from an RCT? I haven't seen an example of that.
--->

An example of a good hypothesis:

* **H0: Groups with a higher adoption percentage of solar lights will have no difference in average percentage repaid by the end of the season**

<!---
What is a higher adoption percentage? Isn't that self selection? Which farmers are we talking about? In which country and district?
These need to be stronger examples. I want people writing more exact hypotheses about where and with whom we expect change to occur.
See this presentation on writing hypotheses for ideas: https://drive.google.com/a/oneacrefund.org/file/d/0B8h28h6EIayDSk1SUTZ4NHktY2s/view?usp=sharing
--->

* **H1: Groups with a higher adoption percentage of solar lights will have a higher average percentage repaid at the end of the season**

<!---
Do you want to introduce one way and two way hypothesis testing? This is a one way hypothesis, we can optimize sample size and power if we set up one way comparisons instead of two, etc.
---->

Note that it is a clear and simple hypothesis, it is easily testable (by distributing solar lights randomly) and specifies *how* we will test for differences (group level average repayment). 

A bad hypothesis example would be be:

* **H0: Ladybugs are a not good natural pesticide for treating aphid infected plants **

* **H1: Ladybugs are a good natural pesticide for treating aphid infected plants **

Why is this so bad? Take a moment to think before expanding the answer below:

## Quick quiz

<div id="spoiler3" style="display:none">

* There is no clear definition of "good natural pesticide" what does being a good pesticide mean? We can imagine a pesticide that destroys 100% of pests but also 100% of crops. Is this a good pesticide?

* How would we randomize this? It is difficult to randomly distribute ladybugs in one area, and to prevent ladybugs being in another area!

* There is no clear way to measure this!


</div>

<button title="Click to show answer" type="button" onclick="if(document.getElementById('spoiler3') .style.display=='none') {document.getElementById('spoiler3') .style.display=''}else{document.getElementById('spoiler3') .style.display='none'}">Show/hide</button>

<!---
I'd focus instead on good examples of strong hypotheses. I removed the untestable hypothesis example.
--->

## Summary

* An RCT requires a clear and well defined hypothesis that is testable. Often a hypothesis will start with a question ("What does solar do to repayment?"), it is the role of the analyst to convert this question into a robust hypothesis with an agreed method of testing. 

* Once we have a hypothesis we need an estimate of the effect size we expect to see, this estimate is directly relevant to sample size calculations (remember a small sample size that is too small makes your results unreliable!). Our estimate can come from historical One Acre Fund data or from well-regarded literature (e.g. from peer-reviewed publications). 

# Hypothesis testing

<!---
If you're not showing much code on how to set up hypothesis tests, do you just want to save this for the next lesson? Right now it doesn't give the reader much actionable to do.

If you want to keep it, I'd flesh out the exact sections on hypothesis testing from lesson 1 you want people to focus on and how they would relate to the hypothesis examples and randomization examples you've given above.
--->

Once we have a testable hypothesis and a metric (see below), we will use a hypothesis test to detect differences in the underlying populations represented by our control and treatment samples [AMP lesson 1](https://michael-bar.github.io/AMP-1-distributions/AMP_distributions_lesson.html#). A hypothesis test will yield a P-value, which is the probability that the populations behind our samples are different. When using a hypothesis test we must set an acceptable level of risk. The acceptable level of risk is known as the p-value threshold, or the alpha level. The most common p-value threshold is 0.05. This means that we are willing to accept a 5% risk of rejecting the null hypothesis incorrectly. In other words, 5% of the time we will conclude there is no difference between our treatments when in fact there is. 

In some cases we might want to set a threshold of 0.01 (1%) or 0.1 (10%). Generally speaking, the more unwilling we are to be incorrect, the lower the threshold. So for an intervention that might have adverse effects on farmers, we would want to be very sure of the positive effects (0.01 threshold) and unwilling to accept negative effects (0.1 threshold). Note that the lower our threshold the larger the sample size needed to detect any effect. 

The principles of power and sample size, that we covered in [AMP lesson 1](https://michael-bar.github.io/AMP-1-distributions/AMP_distributions_lesson.html#), are *incredibly* important here. Low power or sample size will mean that we have very little chance of actually detecting an effect whether it exists or not, furthermore, with low power we will not be able to trust any significant results we do find. Remember that power and sample size calculations require an estimate of the effect size, we can usually obtain an estimate from historical data (what did a regression analysis of historical data suggest the impact on repayment of solar ownership was?) or from literature (ideally peer reviewed sources). 

It is possible to skip a hypothesis test and proceed directly to regression results (lesson 3). However it is a good sanity check to make sure that our regression results match our hypothesis test in an RCT. If these results do not match then it suggests that our RCT was designed poorly. 


## Summary

* We must also set a threshold for our P-value. This threshold represents the level of risk we are willing to accept in being wrong. Note that being more stringent (i.e. lower thresholds) will require a larger sample size!

# Measurement

<!---
As you say, chronologically, measurement precedes hypothesis testing. I'd move this ahead of hypothesis testing.
--->

The final part of the RCT is the measurement itself. This is an often neglected part of RCT design and so usually the source of most problems later on. A hypothesis needs to have a good measurement strategy to be useful. 

I want to take a tangent to describe some analysis I did on an RCT looking at airtime usage in Malawi for a telecommunication company. The RCT examined the effect of an intervention on airtime usage. During the RCT, we asked people to recall how much money they had spent in the last month on airtime, we also had data from the telecommunications company on the *actual* amount spent by the same customers. When we compared the two metrics, there was very little correlation! Furthermore, when we looked at who was most likely to over-estimate their airtime spend, we found it was young, urban males.

Now consider -  had we only had the self-reported data, we would have thought that young urban men were big spenders on airtime and drawn many conclusions from this and found many "statistically significant" (in terms of P-values of < 0.05) relationships! We would have made many recommendations to the telecommunications company and all our data would have held up to statistical interrogation. In short - there would have been very little way to know we were wrong!

The moral of this story is that the best statistics in the world will not save a trial from poor measurement. The other moral is that self-reported data is often terrible, as it is influenced by how people want to be perceived by the interviewer or by their community, or by embarrassment about "low" airtime spends. 

Now, if we were trying to understand the effect of increased yield on farmer food spending, how might we go about it?

Our first ideas might be:

* Weekly enumerators visits to trial participants to ask them to recall their last weeks spend on food.

* Daily spend diary kept by the farmers outlining where they spend their money every day. We then collect the diary after the trial

<!---
Add another quiz section to prompt readers to think about why these may not be good options.
Add more ideas for them to test?
--->

But both these ideas might suffer from the same bias as my airtime study anecdote. This kind of bias is known as **measurement bias** and might arise because some farmers are more forgetful than others, or because some farmers feel social pressure to inflate/deflate their numbers. In this case it requires some careful thought about how to obtain meaningful data. 

How would you improve this trial so the data was more robust (feel free to email ideas to me at [Mike.Barber@](mailto: mike.barber@oneacrefund.org))?

My thoughts so far:

* We could try to get mobile money data from the farmer or from a third-party (in countries where mobile money is common)

* We could have the enumerator visit farmers twice and then see if the answers match

* We could compare answers given to enumerators with third-party data, such as government census data. 

For more materials on the importance of good measurement and measurement strategies in an RCT context, see [teh PRB Trial design guide](https://drive.google.com/open?id=0B8h28h6EIayDOFRHTUFIRXZzbG8)

There are no easy answers to this question, and the solution is likely to be highly dependent on the question being asked and the country it is asked in. However, there is an option available to us to test our data collection methods before we launch a survey called pre-testing.

<!---
Add more details on why even these ideas might not be good? ie. too expensive to visit people each week?

What do you want people to do after reading this section? How are you leading them to better measurement practices? In many ways, pre-testing is the answer because it shows us how our questions or methods might not be useful.
--->
## Pre-test, Pre-test and Pre-test again

Once we have a robust question, and a strategy to measure answers, we will want to **pre-test** the survey.

I cannot emphasize the importance of pre-testing questions enough. Pre-testing is the process through which we draft questions, ask real farmers those questions and assess the extent to which the question elicited the intended response. When pre-testing, we want to consider:

* Are the questions on the survey understood easily by farmers? 

* Are the questions interpreted the same way every time? e.g. "How much fertilizer do you use?" can be answered in terms of Kgs or money. We want to make sure we only get one type of answer!

* Do we want to provide limits on acceptable answers? e.g. "How many acres do you own?" with an answer of "100" is likely to be an error. We can set limits on answers to prevent these sorts of errors. 

Pre-testing allows us to identify questions that seem clear to us as analysts but are understood completely differently by farmers. Pre-testing will also allow us to derive a list of responses that is comprehensive. So if we ask:

* "What livestock do you keep?"

We can make sure that the options (cows, pigs, goats, chickens, other) are available to minimize the amount of "other" that we get.

The final key to great measurement is knowing how the data will be collected and stored. Will you send out paper surveys? If so what happens to the surveys after data entry, and how do we double-check data entry? Once we have the electronic data, where will it be stored so that it is accessible to future One Acre Fund staff? If you are collecting data on a tablet, will you use [CommCare](https://www.commcarehq.org)? 

At a minimum, I would expect all data to be well organized and available on the One Acre Fund Google drive folder. Likewise I would expect all analysis scripts to be deposited in the same folder and clearly linked to the raw data (we will cover this more in future lessons). 

See [Data Management Principles](https://drive.google.com/a/oneacrefund.org/file/d/0B8h28h6EIayDb3cyM1BTSDAtREE/view?usp=sharing) for additional information and guidance.

## Summary

<!---
Say more than think about how to get robust data and measurement. Point to specific practices you want people to follow. Preview the data checking you go on to discuss
--->

* Think about how to get robust and reliable data. Think about how to make sure we can trust the data.

* Pre-test surveys to find bugs and errors! Even if you think it is perfect, pre-test!

* Have a clear plan for where to store raw data and scripts. We need this to be accessible to other One Acre Fund analysts to preserve your work for future staff!

<!---
Have you mentioned scripts? Will it be clear what you mean?
--->


# Data checking

During the course of the RCT it is important to check data as it comes in. This will enable you to identify any issues early on and (hopefully) fix them before they become chronic. During data checking we are looking to see whether our survey is capturing the data expected. Some One Acre Fund examples of where this has been useful:

* Plot measurements by enumerators showing a total acreage of 0.001 acres. This is likely an enumeration or data validation error.

* Questions abour fertiliser use being answered both in kilograms and Kenyan shillings.

In these cases we are really enquiring about **outliers**.

<!----
It's outliers and it's data entry errors. I think we're concerned about both. An outlier can be a true outlier but a real value. A mistake is just a mistake though. We want to follow up with both.
--->

An outlier is a data point which lies outside the main distribution and may signal data recorded or entered incorrectly. The problem with outliers is trying to distinguish real data that is just unexpected (but is valid) and data which is incorrect due to enumerator survey error. 

We can visualize data with a beeswarm plot, note the potential outliers at the top of the plot:

```{r outl}
set.seed(112)
#make some data
df = data.frame(ID=seq(1,100), vals=rnorm(100,100,100))
#add some outliers
df$vals[98:100] = rnorm(3,1000,1000)


library(ggbeeswarm)
ggplot(df) +  geom_beeswarm(aes(x=1,y=vals)) + ggtitle("Beeswarm of simulated data") 



```

We can also try to define outliers mathematically, the boxplot uses a simple formula to identify potential outliers, I have included a function below that will use this logic to find outliers for numerical values:

<!---
I'd explain the function and show how to apply it to multiple columns in the data. 
What sort of output are they looking for? 
Does this work for all variables regardless of distribution?

In addition to putting these functions in the AMP lessons, if you want these to be the functions people use, I'd start loading them to a github 'analysis' repo from which analysts can just devtools::github_install(repo_url) and have all the functions available to them.
--->

```{r out2}

#this function expects a vector of numeric values
find_outliers <- function(data_to_test){

      IQR<- quantile(data_to_test, 0.75, na.rm=TRUE)[[1]] - quantile(data_to_test, 0.25, na.rm=TRUE)[[1]]
      cutoff = subset(data_to_test, data_to_test <=  quantile(data_to_test, 0.25, na.rm=TRUE)-(IQR*1.5) )  
      cutoff2 = subset(data_to_test, data_to_test >=   quantile(data_to_test, 0.75, na.rm=TRUE)+(IQR*1.5) ) 
      ret = c(cutoff,cutoff2)
      return(ret)
}

find_outliers(df$vals)


```


These tools can help us identify potential outliers but cannot definitively tell us whether a data point is real or not. When we assign something as an outlier we are making a big assumption about the underlying distribution of data and what *we* think it should look like. There is always a danger that our assumption is wrong or that we have biases leading us to incorrectly label things as outliers.

If we are confident that a value is mistaken (and not just a true extreme) then we can replace that value with NA, to indicate that it is missing. This will mean subsequent calculations will be done without that value. 

There is no easy solution to this, and often it will be contextual knowledge which we use to identify real outliers, for example, it is likely that a farmer reporting 10000 Kg of fertilizer per acre is actually a case of enumerator error. You can read more about outliers [here](https://drive.google.com/file/d/0Bz09wJM9mJC_M2kwZUIzSmFzNlE/view) and [here](https://oneacrefund.igloocommunities.com/teams/global/agresearch/documents/forms/analyst_mentorship_program/amp_lesson_2pdf)

## Summary

* Data checking ensures that come analysis time, our data is free from outliers and issues

* Outliers are often difficult to define - remember the 68-95-99.7 rule from lesson 1. In a normal distribution of values we expect 68% of the values to be within 1 stdev of the mean. 95% to be within 2 stdev and 99.7% to be within 3 stdev of the mean. This means we can have fairly extreme values that are still well within what we expect for a normal distribution. 

# Summary

We have now seen the key parts of an RCT:

* **Hypothesis**: A good hypothesis is *testable* and *measurable*. It must have a clearly defined evaluation criteria (e.g. are we measuring the average or the median? At the individual or group level?). 

* **Randomization**: We must randomize using R and ensuring our Treatment and Control groups are balanced. We can use my RCT_random function to achieve this. We must also remember cluster effects and use the correct *unit of randomization*. It is possible to calculate the ICC and adjust sample size accordingly, it is also fair to assume an ICC of 1.0 and have the sample size calculated from the number of clusters (i.e. calculate the number of groups rather than individuals).

* **Power**: Know your statistical power and sample size before and after a trial - see [AMP lesson 1](https://michael-bar.github.io/AMP-1-distributions/AMP_distributions_lesson.html#). We will need an estimate of effect size from historic data or peer-reviewed literature. 

* **Measurement**: Measurements must be robust and reliable, think about the ways that we might be inaccurate and try to minimize the important of self-reported metrics in any study you design!

* **Pre-test**: Pre-test all questions with real farmers to understand how *they* understand your survey and look for difficulties. Remember - a poor survey is due to the analysts bad communication, not the farmers bad interpretation! 

* Combining lesson 1 (distributions, powers, p-values) and lesson 2 (RCT principles) should now mean you are able to start designing RCTs! Our next few lessons will look at how to analyse data effectively using hypothesis testing and regressions. 


# Additional reading

* [The RCT vs. other trials at One Acre Fund](https://docs.google.com/presentation/d/14CpKbrEtFOzPfYTmfwKZgnpmANYbKC8YmX1ZxTRZsQs/edit#slide=id.p4)

* [Effective hypothesis design](https://www.producttalk.org/2014/11/the-5-components-of-a-good-hypothesis)

* [Self-reporting pitfalls (reccomended!)](http://www.sciencebrainwaves.com/the-dangers-of-self-report/)



